{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8829388,"sourceType":"datasetVersion","datasetId":5312326},{"sourceId":8831197,"sourceType":"datasetVersion","datasetId":5313720},{"sourceId":137044,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":115997,"modelId":139227}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# /kaggle/input/car-crash-dataset-ccd/CrashBest\n# /kaggle/input/car-crash-dataset-ccd/CrashBest/C_000001_01.jpg\n# /kaggle/input/crash-1500/Crash-1500/000001.mp4\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T13:50:37.766324Z","iopub.status.idle":"2024-07-16T13:50:37.767145Z","shell.execute_reply.started":"2024-07-16T13:50:37.766865Z","shell.execute_reply":"2024-07-16T13:50:37.766885Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install ffmpeg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-11T13:49:23.831211Z","iopub.execute_input":"2024-10-11T13:49:23.831617Z","iopub.status.idle":"2024-10-11T13:49:33.064224Z","shell.execute_reply.started":"2024-10-11T13:49:23.831579Z","shell.execute_reply":"2024-10-11T13:49:33.063308Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file_path = '/kaggle/input/preprocessed-df/preprocessed_df (1).csv'\nannotations = pd.read_csv(csv_file_path)\n\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T10:59:58.675397Z","iopub.execute_input":"2024-11-13T10:59:58.675988Z","iopub.status.idle":"2024-11-13T10:59:58.700657Z","shell.execute_reply.started":"2024-11-13T10:59:58.675955Z","shell.execute_reply":"2024-11-13T10:59:58.699761Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Severity of the Crash\nModerate    622\nMinor       348\nSevere      244\nmoderate     70\nsevere       57\nminor        31\nfatal        25\nFatal        18\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.lower()\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:01:04.369517Z","iopub.execute_input":"2024-11-13T11:01:04.370306Z","iopub.status.idle":"2024-11-13T11:01:04.378455Z","shell.execute_reply.started":"2024-11-13T11:01:04.370272Z","shell.execute_reply":"2024-11-13T11:01:04.377452Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      301\nfatal        43\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nvideo_dir = '/kaggle/input/crash-1500/Crash-1500'\nframes_dir = '/kaggle/working/frames' \n\nfor idx, row in annotations.iterrows():\n    video_number = row['Video Number']\n    video_path = os.path.join(video_dir, f'{video_number:06}.mp4')\n    output_dir = os.path.join(frames_dir, f'{video_number:06}')\n    os.makedirs(output_dir, exist_ok=True)\n\n    ffmpeg_command = f'ffmpeg -ss 00:00:01 -i \"{video_path}\" -vf fps=5 \"{output_dir}/frame%04d.png\"'\n#     print(f\"Running FFmpeg command: {ffmpeg_command}\")\n    os.system(ffmpeg_command)\n    \n    extracted_frames = os.listdir(output_dir)\n    print(f\"Extracted {len(extracted_frames)} frames for video {video_number}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nframes_dir = '/kaggle/working/frames'\nvideo_dirs = next(os.walk(frames_dir))[1]\nnum_videos = len(video_dirs)\n\nprint(f\"Number of videos with extracted frames: {num_videos}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:02:32.016077Z","iopub.execute_input":"2024-11-13T11:02:32.016480Z","iopub.status.idle":"2024-11-13T11:02:32.026268Z","shell.execute_reply.started":"2024-11-13T11:02:32.016449Z","shell.execute_reply":"2024-11-13T11:02:32.025376Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Number of videos with extracted frames: 1415\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass VideoDataset(Dataset):\n    def __init__(self, annotations_df, root_dir, transform=None, num_frames=20):\n        self.annotations = annotations_df\n        self.annotations['Severity of the Crash'] = self.annotations['Severity of the Crash'].str.lower()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.label_map = {'minor': 0, 'moderate': 1, 'severe': 2,'fatal': 3}\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        video_number = self.annotations.iloc[idx]['Video Number']\n        label = self.label_map[self.annotations.iloc[idx]['Severity of the Crash']]\n        frame_dir = os.path.join(self.root_dir, f'{video_number:06}')\n        frames = []\n        # print(f\"Looking for frames in: {frame_dir}\")\n        for frame_path in sorted(glob.glob(os.path.join(frame_dir, '*.png'))):\n            frame = read_image(frame_path)\n            frame = transforms.ToPILImage()(frame)\n\n            if self.transform:\n                frame = self.transform(frame)\n            frames.append(frame)\n        # print(f\"Found {len(frames)} frames.\")\n\n        if not frames: \n            raise ValueError(f\"No frames found for video {video_number} in directory {frame_dir}\")\n        frames = torch.stack(frames)\n        return frames, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.RandomHorizontalFlip(),  \n    transforms.RandomRotation(10), \n    transforms.ToTensor(),  \n])\n\ndataset = VideoDataset(annotations_df=annotations, root_dir='/kaggle/working/frames', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:02:35.730097Z","iopub.execute_input":"2024-11-13T11:02:35.730769Z","iopub.status.idle":"2024-11-13T11:02:40.516700Z","shell.execute_reply.started":"2024-11-13T11:02:35.730735Z","shell.execute_reply":"2024-11-13T11:02:40.515903Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom transformers import VideoMAEConfig, VideoMAEModel\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:02:54.077978Z","iopub.execute_input":"2024-11-13T11:02:54.078547Z","iopub.status.idle":"2024-11-13T11:02:54.992600Z","shell.execute_reply.started":"2024-11-13T11:02:54.078514Z","shell.execute_reply":"2024-11-13T11:02:54.991835Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class VideoClassifier(nn.Module):\n    def __init__(self, num_classes, num_frames):\n        super().__init__()\n        self.config = VideoMAEConfig(\n            num_frames=num_frames,\n            num_classes=num_classes\n        )\n        self.videomae = VideoMAEModel(self.config)\n        self.classifier = nn.Linear(self.config.hidden_size, num_classes)\n\n    def forward(self, x):\n        outputs = self.videomae(x)\n        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n        return logits\n\nmodel = VideoClassifier(num_classes=4, num_frames=20)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:03:01.239128Z","iopub.execute_input":"2024-11-13T11:03:01.239694Z","iopub.status.idle":"2024-11-13T11:03:08.171097Z","shell.execute_reply.started":"2024-11-13T11:03:01.239659Z","shell.execute_reply":"2024-11-13T11:03:08.170239Z"},"trusted":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"VideoClassifier(\n  (videomae): VideoMAEModel(\n    (embeddings): VideoMAEEmbeddings(\n      (patch_embeddings): VideoMAEPatchEmbeddings(\n        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n      )\n    )\n    (encoder): VideoMAEEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x VideoMAELayer(\n          (attention): VideoMAESdpaAttention(\n            (attention): VideoMAESdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=False)\n              (key): Linear(in_features=768, out_features=768, bias=False)\n              (value): Linear(in_features=768, out_features=768, bias=False)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): VideoMAESelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): VideoMAEIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): VideoMAEOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (classifier): Linear(in_features=768, out_features=4, bias=True)\n)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"class_counts = annotations['label'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:03:14.218062Z","iopub.execute_input":"2024-11-13T11:03:14.218460Z","iopub.status.idle":"2024-11-13T11:03:14.226231Z","shell.execute_reply.started":"2024-11-13T11:03:14.218428Z","shell.execute_reply":"2024-11-13T11:03:14.225032Z"},"trusted":true},"outputs":[{"name":"stdout","text":"label\n1    692\n0    379\n2    301\n3     43\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, SubsetRandomSampler\nfrom collections import Counter\n\nnp.random.seed(42)\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices)\n\ntrain_split = int(np.floor(0.7 * dataset_size))\nval_split = int(np.floor(0.2 * dataset_size))\ntest_split = dataset_size - train_split - val_split\n\ntrain_indices = indices[:train_split]\nval_indices = indices[train_split:train_split + val_split]\ntest_indices = indices[train_split + val_split:]\n\nprint(f\"Total dataset size: {dataset_size}\")\nprint(f\"Training set size: {len(train_indices)}\")\nprint(f\"Validation set size: {len(val_indices)}\")\nprint(f\"Test set size: {len(test_indices)}\")\n\ntrain_sampler = SubsetRandomSampler(train_indices)\nval_sampler = SubsetRandomSampler(val_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrain_loader = DataLoader(dataset, batch_size=4, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=4, sampler=val_sampler)\ntest_loader = DataLoader(dataset, batch_size=4, sampler=test_sampler)\n\nprint(\"Total effective samples in training set:\", len(train_loader) * train_loader.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:03:18.646803Z","iopub.execute_input":"2024-11-13T11:03:18.647167Z","iopub.status.idle":"2024-11-13T11:03:18.657582Z","shell.execute_reply.started":"2024-11-13T11:03:18.647137Z","shell.execute_reply":"2024-11-13T11:03:18.656570Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Total dataset size: 1415\nTraining set size: 990\nValidation set size: 283\nTest set size: 142\nTotal effective samples in training set: 992\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 4\nbest_val_acc = 0.0  \nsave_path = '/kaggle/working/best_model.pth'  \n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for data in train_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Here the model is evaluated on the validation set\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n\n    print('[%d] loss: %.3f, val_acc: %.3f' % (epoch + 1, running_loss / len(train_loader), val_acc))\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with val_acc: {best_val_acc:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T11:03:22.369834Z","iopub.execute_input":"2024-11-13T11:03:22.370242Z","iopub.status.idle":"2024-11-13T12:03:16.656292Z","shell.execute_reply.started":"2024-11-13T11:03:22.370186Z","shell.execute_reply":"2024-11-13T12:03:16.655182Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[1] loss: 5.726, val_acc: 29.682\nNew best model saved with val_acc: 29.682\n[2] loss: 1.477, val_acc: 50.177\nNew best model saved with val_acc: 50.177\n[3] loss: 1.333, val_acc: 38.869\n[4] loss: 1.336, val_acc: 33.569\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = 100 * correct / total\nprint('Test Accuracy: %.3f' % test_acc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T12:07:31.227719Z","iopub.execute_input":"2024-11-13T12:07:31.228127Z","iopub.status.idle":"2024-11-13T12:08:26.815490Z","shell.execute_reply.started":"2024-11-13T12:07:31.228090Z","shell.execute_reply":"2024-11-13T12:08:26.814503Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 52.113\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\ndef calculate_metrics(model, data_loader, device):\n    model.eval()\n    correct_predictions = 0\n    custom_correct = 0\n    all_labels = []\n    all_preds = []\n\n    with torch.no_grad():\n        for data in data_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            # The probabilities from the outputs\n            probabilities = torch.softmax(outputs, dim=1)\n            top_probs, top_classes = torch.topk(probabilities, 2, dim=1)\n\n            p1 = top_classes[:, 0]  \n            p2 = top_classes[:, 1]  \n\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(p1.cpu().numpy())\n\n            for i in range(len(labels)):\n                true_label = labels[i].item()\n                predicted1 = p1[i].item()\n                predicted2 = p2[i].item()\n                \n                if abs(predicted1-predicted2) == 1:\n                    if(predicted1==true_label or predicted2==true_label):  \n                        custom_correct += 1 \n                else:\n                    if predicted1 == true_label:\n                        custom_correct += 1\n\n    total_samples = len(all_labels)\n    custom_accuracy = custom_correct / total_samples\n    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n\n    print(f\"Custom Accuracy: {custom_accuracy * 100:.2f}%\")\n    print(f\"Precision: {precision:.2f}\")\n    print(f\"Recall: {recall:.2f}\")\n    print(f\"F1 Score: {f1:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-11-13T12:08:58.349191Z","iopub.execute_input":"2024-11-13T12:08:58.349578Z","iopub.status.idle":"2024-11-13T12:08:59.037178Z","shell.execute_reply.started":"2024-11-13T12:08:58.349549Z","shell.execute_reply":"2024-11-13T12:08:59.036432Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model.load_state_dict(torch.load(save_path))\ncalculate_metrics(model, test_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-13T12:09:02.953739Z","iopub.execute_input":"2024-11-13T12:09:02.954492Z","iopub.status.idle":"2024-11-13T12:09:59.040514Z","shell.execute_reply.started":"2024-11-13T12:09:02.954449Z","shell.execute_reply":"2024-11-13T12:09:59.039549Z"}},"outputs":[{"name":"stdout","text":"Custom Accuracy: 74.65%\nPrecision: 0.27\nRecall: 0.52\nF1 Score: 0.36\n","output_type":"stream"}],"execution_count":14}]}