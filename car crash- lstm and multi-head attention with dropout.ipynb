{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8829388,"sourceType":"datasetVersion","datasetId":5312326},{"sourceId":8831197,"sourceType":"datasetVersion","datasetId":5313720}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# /kaggle/input/car-crash-dataset-ccd/CrashBest\n# /kaggle/input/car-crash-dataset-ccd/CrashBest/C_000001_01.jpg\n# /kaggle/input/crash-1500/Crash-1500/000001.mp4\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T13:50:37.766324Z","iopub.status.idle":"2024-07-16T13:50:37.767145Z","shell.execute_reply.started":"2024-07-16T13:50:37.766865Z","shell.execute_reply":"2024-07-16T13:50:37.766885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install ffmpeg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-10-10T13:36:12.395176Z","iopub.execute_input":"2024-10-10T13:36:12.395504Z","iopub.status.idle":"2024-10-10T13:36:21.754705Z","shell.execute_reply.started":"2024-10-10T13:36:12.395477Z","shell.execute_reply":"2024-10-10T13:36:21.753411Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Get:1 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1227 B]\nGet:2 https://packages.cloud.google.com/apt cloud-sdk InRelease [1618 B]       \nGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nGet:4 https://packages.cloud.google.com/apt google-fast-socket InRelease [1071 B]\nHit:5 http://archive.ubuntu.com/ubuntu focal InRelease                         \nGet:6 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\nGet:7 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [27.8 kB]\nGet:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nGet:9 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1546 kB]\nGet:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1726 kB]\nGet:11 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3321 kB]\nGet:12 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [15.4 kB]\nHit:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease              \nGet:14 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [4036 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [4188 kB]\nGet:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1274 kB]\nGet:17 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [4025 kB]\nGet:19 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:20 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4488 kB]\nGet:21 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1561 kB]\nFetched 26.5 MB in 3s (8724 kB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n0 upgraded, 0 newly installed, 0 to remove and 105 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file_path = '/kaggle/input/preprocessed-df/preprocessed_df (1).csv'\nannotations = pd.read_csv(csv_file_path)\n\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:36:26.636388Z","iopub.execute_input":"2024-10-10T13:36:26.637362Z","iopub.status.idle":"2024-10-10T13:36:27.143494Z","shell.execute_reply.started":"2024-10-10T13:36:26.637313Z","shell.execute_reply":"2024-10-10T13:36:27.141788Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Severity of the Crash\nModerate    622\nMinor       348\nSevere      244\nmoderate     70\nsevere       57\nminor        31\nfatal        25\nFatal        18\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.lower()\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:36:30.170515Z","iopub.execute_input":"2024-10-10T13:36:30.170891Z","iopub.status.idle":"2024-10-10T13:36:30.179794Z","shell.execute_reply.started":"2024-10-10T13:36:30.170862Z","shell.execute_reply":"2024-10-10T13:36:30.178585Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      301\nfatal        43\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nvideo_dir = '/kaggle/input/crash-1500/Crash-1500'\nframes_dir = '/kaggle/working/frames' \n\nfor idx, row in annotations.iterrows():\n    video_number = row['Video Number']\n    video_path = os.path.join(video_dir, f'{video_number:06}.mp4')\n    output_dir = os.path.join(frames_dir, f'{video_number:06}')\n    os.makedirs(output_dir, exist_ok=True)\n\n    ffmpeg_command = f'ffmpeg -ss 00:00:01 -i \"{video_path}\" -vf fps=5 \"{output_dir}/frame%04d.png\"'\n#     print(f\"Running FFmpeg command: {ffmpeg_command}\")\n    os.system(ffmpeg_command)\n    \n    extracted_frames = os.listdir(output_dir)\n    print(f\"Extracted {len(extracted_frames)} frames for video {video_number}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFilter,ImageEnhance\nimport cv2\nimport numpy as np\n\ndef denoise_frame(frame):\n    frame = np.array(frame)\n    denoised_frame = cv2.medianBlur(frame, 5)\n    return Image.fromarray(denoised_frame)\n\ndef deblur_frame(frame):\n    return frame.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n\ndef enhance_contrast(frame):\n    frame = np.array(frame)\n    lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl, a, b))\n    enhanced_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    return Image.fromarray(enhanced_frame)\n\ndef sharpen_frame(frame, factor=2.0):\n    enhancer = ImageEnhance.Sharpness(frame)\n    return enhancer.enhance(factor)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:49:12.305957Z","iopub.execute_input":"2024-10-10T13:49:12.306948Z","iopub.status.idle":"2024-10-10T13:49:12.601658Z","shell.execute_reply.started":"2024-10-10T13:49:12.306895Z","shell.execute_reply":"2024-10-10T13:49:12.600663Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import os\nframes_dir = '/kaggle/working/frames'\nvideo_dirs = next(os.walk(frames_dir))[1]\nnum_videos = len(video_dirs)\n\nprint(f\"Number of videos with extracted frames: {num_videos}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:49:16.645952Z","iopub.execute_input":"2024-10-10T13:49:16.646913Z","iopub.status.idle":"2024-10-10T13:49:16.655336Z","shell.execute_reply.started":"2024-10-10T13:49:16.646874Z","shell.execute_reply":"2024-10-10T13:49:16.654323Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of videos with extracted frames: 1415\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport glob\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom PIL import Image\n\nclass VideoDataset(Dataset):\n    def __init__(self, annotations_df, root_dir, transform=None, num_frames=20):\n        self.annotations = annotations_df\n        self.annotations['Severity of the Crash'] = self.annotations['Severity of the Crash'].str.lower()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.label_map = {'minor': 0, 'moderate': 1, 'severe': 2,'fatal': 3}\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        video_number = self.annotations.iloc[idx]['Video Number']\n        label = self.label_map[self.annotations.iloc[idx]['Severity of the Crash']]\n        frame_dir = os.path.join(self.root_dir, f'{video_number:06}')\n        frames = []\n        # print(f\"Looking for frames in: {frame_dir}\")\n        for frame_path in sorted(glob.glob(os.path.join(frame_dir, '*.png'))):\n            frame = read_image(frame_path)\n            frame = transforms.ToPILImage()(frame)\n            frame = denoise_frame(frame)\n            frame = sharpen_frame(frame)\n            frame = enhance_contrast(frame)\n            frame = deblur_frame(frame)\n            if self.transform:\n                frame = self.transform(frame)\n            frames.append(frame)\n        # print(f\"Found {len(frames)} frames.\")\n\n        if not frames: \n            raise ValueError(f\"No frames found for video {video_number} in directory {frame_dir}\")\n        frames = torch.stack(frames)\n        return frames, label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.RandomHorizontalFlip(),  \n    transforms.RandomRotation(10), \n    transforms.ToTensor(),  \n])\n\ndataset = VideoDataset(annotations_df=annotations, root_dir='/kaggle/working/frames', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:49:20.345739Z","iopub.execute_input":"2024-10-10T13:49:20.346131Z","iopub.status.idle":"2024-10-10T13:49:26.484618Z","shell.execute_reply.started":"2024-10-10T13:49:20.346079Z","shell.execute_reply":"2024-10-10T13:49:26.483762Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import TimesformerModel, TimesformerConfig\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:49:35.825300Z","iopub.execute_input":"2024-10-10T13:49:35.826718Z","iopub.status.idle":"2024-10-10T13:49:37.089020Z","shell.execute_reply.started":"2024-10-10T13:49:35.826673Z","shell.execute_reply":"2024-10-10T13:49:37.088097Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import TimesformerModel, TimesformerConfig\n\nclass VideoClassifier(nn.Module):\n    def __init__(self, num_classes, num_frames, hidden_size=512, num_layers=1, bidirectional=True, num_heads=4, dropout=0.2):\n        super().__init__()\n        self.config = TimesformerConfig(\n            num_frames=num_frames,\n            num_classes=num_classes,\n            hidden_dropout_prob=dropout,  \n            attention_probs_dropout_prob=dropout,  \n            drop_path_rate=dropout  \n        )\n        self.timesformer = TimesformerModel(self.config)\n        self.lstm = nn.LSTM(\n            input_size=self.config.hidden_size, \n            hidden_size=hidden_size, \n            num_layers=num_layers, \n            bidirectional=bidirectional, \n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0\n        )\n        lstm_output_size = hidden_size * 2 if bidirectional else hidden_size\n\n        # Multihead attention mechanism\n        self.multihead_attention = nn.MultiheadAttention(\n            embed_dim=lstm_output_size, \n            num_heads=num_heads, \n            dropout=dropout,\n            batch_first=True\n        )\n        \n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(lstm_output_size, num_classes)\n\n    def forward(self, x):\n        transformer_output = self.timesformer(x).last_hidden_state  \n        lstm_output, _ = self.lstm(transformer_output)          \n        \n        # Applying multihead attention\n        attn_output, _ = self.multihead_attention(lstm_output, lstm_output, lstm_output)\n        attn_output = self.dropout(attn_output)          \n        context_vector = torch.sum(attn_output, dim=1)       \n        logits = self.classifier(context_vector) \n        return logits\n\nmodel = VideoClassifier(num_classes=4, num_frames=20, dropout=0.2)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:49:41.365756Z","iopub.execute_input":"2024-10-10T13:49:41.366324Z","iopub.status.idle":"2024-10-10T13:49:44.611107Z","shell.execute_reply.started":"2024-10-10T13:49:41.366283Z","shell.execute_reply":"2024-10-10T13:49:44.610108Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"VideoClassifier(\n  (timesformer): TimesformerModel(\n    (embeddings): TimesformerEmbeddings(\n      (patch_embeddings): TimesformerPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (pos_drop): Dropout(p=0.2, inplace=False)\n      (time_drop): Dropout(p=0.2, inplace=False)\n    )\n    (encoder): TimesformerEncoder(\n      (layer): ModuleList(\n        (0): TimesformerLayer(\n          (drop_path): Identity()\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (1): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.0181818176060915)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (2): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.036363635212183)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (3): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.05454545468091965)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (4): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.072727270424366)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (5): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.09090908616781235)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (6): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.10909091681241989)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (7): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.12727272510528564)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (8): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.1454545557498932)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (9): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.16363637149333954)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (10): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.1818181872367859)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (11): TimesformerLayer(\n          (drop_path): TimeSformerDropPath(p=0.20000000298023224)\n          (attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (intermediate): TimesformerIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): TimesformerOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n          (temporal_attention): TimeSformerAttention(\n            (attention): TimesformerSelfAttention(\n              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n              (attn_drop): Dropout(p=0.2, inplace=False)\n            )\n            (output): TimesformerSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.2, inplace=False)\n            )\n          )\n          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n  )\n  (lstm): LSTM(768, 512, batch_first=True, bidirectional=True)\n  (multihead_attention): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n  )\n  (dropout): Dropout(p=0.2, inplace=False)\n  (classifier): Linear(in_features=1024, out_features=4, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"class_counts = annotations['label'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:49:59.110835Z","iopub.execute_input":"2024-10-10T13:49:59.111217Z","iopub.status.idle":"2024-10-10T13:49:59.122172Z","shell.execute_reply.started":"2024-10-10T13:49:59.111185Z","shell.execute_reply":"2024-10-10T13:49:59.120891Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"label\n1    692\n0    379\n2    301\n3     43\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, SubsetRandomSampler\nfrom collections import Counter\n\nnp.random.seed(21)\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices)\n\ntrain_split = int(np.floor(0.7 * dataset_size))\nval_split = int(np.floor(0.2 * dataset_size))\ntest_split = dataset_size - train_split - val_split\n\ntrain_indices = indices[:train_split]\nval_indices = indices[train_split:train_split + val_split]\ntest_indices = indices[train_split + val_split:]\n\nprint(f\"Total dataset size: {dataset_size}\")\nprint(f\"Training set size: {len(train_indices)}\")\nprint(f\"Validation set size: {len(val_indices)}\")\nprint(f\"Test set size: {len(test_indices)}\")\n\ntrain_labels = [annotations['Severity of the Crash'][idx] for idx in train_indices]\nclass_counts = Counter(train_labels)\ntotal_samples = len(train_labels)\nclass_weights = {cls: total_samples / count for cls, count in class_counts.items()}\nprint(class_weights)\nsample_weights = [class_weights[annotations['Severity of the Crash'][idx]] for idx in train_indices]\nsample_weights = torch.tensor(sample_weights, dtype=torch.float)\n\ntrain_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\nval_sampler = SubsetRandomSampler(val_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\ntrain_loader = DataLoader(dataset, batch_size=2, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=2, sampler=val_sampler)\ntest_loader = DataLoader(dataset, batch_size=2, sampler=test_sampler)\n\nprint(\"Total effective samples in training set:\", len(train_loader) * train_loader.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:50:02.820735Z","iopub.execute_input":"2024-10-10T13:50:02.821144Z","iopub.status.idle":"2024-10-10T13:50:02.863276Z","shell.execute_reply.started":"2024-10-10T13:50:02.821109Z","shell.execute_reply":"2024-10-10T13:50:02.862201Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Total dataset size: 1415\nTraining set size: 990\nValidation set size: 283\nTest set size: 142\n{'minor': 3.680297397769517, 'moderate': 2.0625, 'severe': 4.691943127962086, 'fatal': 33.0}\nTotal effective samples in training set: 990\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CustomPenaltyLoss(nn.Module):\n    def __init__(self, penalty_factor=2.0):\n        super(CustomPenaltyLoss, self).__init__()\n        self.penalty_factor = penalty_factor\n\n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        predictions = torch.argmax(inputs, dim=1)\n        wrong_predictions = (predictions != targets).float()\n        penalty = wrong_predictions * self.penalty_factor\n        penalized_loss = ce_loss + penalty\n        return penalized_loss.mean()","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:50:08.333959Z","iopub.execute_input":"2024-10-10T13:50:08.334806Z","iopub.status.idle":"2024-10-10T13:50:08.342772Z","shell.execute_reply.started":"2024-10-10T13:50:08.334763Z","shell.execute_reply":"2024-10-10T13:50:08.341717Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = CustomPenaltyLoss(penalty_factor=2.0)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 4\nbest_val_acc = 0.0  \nsave_path = '/kaggle/working/best_model.pth'  \n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for data in train_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    # Here the model is evaluated on the validation set\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n\n    print('[%d] loss: %.3f, val_acc: %.3f' % (epoch + 1, running_loss / len(train_loader), val_acc))\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with val_acc: {best_val_acc:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-10T13:50:12.295530Z","iopub.execute_input":"2024-10-10T13:50:12.295896Z","iopub.status.idle":"2024-10-10T17:57:18.161189Z","shell.execute_reply.started":"2024-10-10T13:50:12.295866Z","shell.execute_reply":"2024-10-10T17:57:18.158229Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[1] loss: 361.154, val_acc: 23.322\nNew best model saved with val_acc: 23.322\n[2] loss: 7.789, val_acc: 46.290\nNew best model saved with val_acc: 46.290\n[3] loss: 3.589, val_acc: 23.322\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 18\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     20\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"model.load_state_dict(torch.load('/kaggle/working/best_model.pth'))\nmodel.eval()\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        inputs, labels = data\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\ntest_acc = 100 * correct / total\nprint('Test Accuracy: %.3f' % test_acc)","metadata":{"execution":{"iopub.status.busy":"2024-10-10T17:57:25.835276Z","iopub.execute_input":"2024-10-10T17:57:25.835903Z","iopub.status.idle":"2024-10-10T18:03:50.703233Z","shell.execute_reply.started":"2024-10-10T17:57:25.835868Z","shell.execute_reply":"2024-10-10T18:03:50.702267Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Test Accuracy: 57.042\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()                           \n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T10:40:33.539971Z","iopub.execute_input":"2024-08-01T10:40:33.540449Z","iopub.status.idle":"2024-08-01T10:40:50.284465Z","shell.execute_reply.started":"2024-08-01T10:40:33.540414Z","shell.execute_reply":"2024-08-01T10:40:50.283431Z"},"trusted":true},"execution_count":null,"outputs":[]}]}