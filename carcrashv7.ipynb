{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8829388,"sourceType":"datasetVersion","datasetId":5312326},{"sourceId":8831197,"sourceType":"datasetVersion","datasetId":5313720}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/kshitijpoojary/carcrash?scriptVersionId=189312744\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# /kaggle/input/car-crash-dataset-ccd/CrashBest\n# /kaggle/input/car-crash-dataset-ccd/CrashBest/C_000001_01.jpg\n# /kaggle/input/crash-1500/Crash-1500/000001.mp4\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2024-07-16T13:50:37.766324Z","iopub.status.idle":"2024-07-16T13:50:37.767145Z","shell.execute_reply.started":"2024-07-16T13:50:37.766865Z","shell.execute_reply":"2024-07-16T13:50:37.766885Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get update\n!sudo apt-get install ffmpeg","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-22T05:26:13.0642Z","iopub.execute_input":"2024-07-22T05:26:13.064691Z","iopub.status.idle":"2024-07-22T05:26:21.132838Z","shell.execute_reply.started":"2024-07-22T05:26:13.064662Z","shell.execute_reply":"2024-07-22T05:26:21.131612Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\nGet:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease [128 kB]        \nHit:3 http://archive.ubuntu.com/ubuntu focal-backports InRelease               \nGet:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\nGet:5 https://packages.cloud.google.com/apt gcsfuse-focal InRelease [1225 B]   \nGet:6 https://packages.cloud.google.com/apt cloud-sdk InRelease [1616 B]\nGet:7 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]\nGet:8 https://packages.cloud.google.com/apt google-fast-socket InRelease [1071 B]\nGet:9 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4269 kB]\nGet:10 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1533 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [33.5 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3924 kB]\nGet:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1557 kB]\nGet:14 https://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [24.8 kB]\nGet:15 https://packages.cloud.google.com/apt cloud-sdk/main all Packages [1480 kB]\nGet:16 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [3107 kB]\nGet:17 https://packages.cloud.google.com/apt google-fast-socket/main amd64 Packages [15.4 kB]\nGet:18 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3803 kB]\nGet:19 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [30.9 kB]\nGet:20 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1247 kB]\nGet:21 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3773 kB]\nFetched 25.1 MB in 2s (10.8 MB/s)                           \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n0 upgraded, 0 newly installed, 0 to remove and 92 not upgraded.\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ncsv_file_path = '/kaggle/input/preprocessed-df/preprocessed_df (1).csv'\nannotations = pd.read_csv(csv_file_path)\n\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:26:27.730254Z","iopub.execute_input":"2024-07-22T05:26:27.731175Z","iopub.status.idle":"2024-07-22T05:26:28.157984Z","shell.execute_reply.started":"2024-07-22T05:26:27.731134Z","shell.execute_reply":"2024-07-22T05:26:28.156878Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Severity of the Crash\nModerate    622\nMinor       348\nSevere      244\nmoderate     70\nsevere       57\nminor        31\nfatal        25\nFatal        18\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.lower()\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:26:32.655208Z","iopub.execute_input":"2024-07-22T05:26:32.655641Z","iopub.status.idle":"2024-07-22T05:26:32.666391Z","shell.execute_reply.started":"2024-07-22T05:26:32.655604Z","shell.execute_reply":"2024-07-22T05:26:32.665237Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      301\nfatal        43\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"annotations['Severity of the Crash'] = annotations['Severity of the Crash'].str.replace('fatal', 'severe', case=False)\nannotations['label'] = annotations['label'].replace(3, 2)\nclass_counts = annotations['Severity of the Crash'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:26:36.985242Z","iopub.execute_input":"2024-07-22T05:26:36.985623Z","iopub.status.idle":"2024-07-22T05:26:36.995816Z","shell.execute_reply.started":"2024-07-22T05:26:36.985589Z","shell.execute_reply":"2024-07-22T05:26:36.994778Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Severity of the Crash\nmoderate    692\nminor       379\nsevere      344\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"unique_labels = annotations['label'].unique()\nprint(unique_labels)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:26:40.216245Z","iopub.execute_input":"2024-07-22T05:26:40.217199Z","iopub.status.idle":"2024-07-22T05:26:40.223573Z","shell.execute_reply.started":"2024-07-22T05:26:40.217163Z","shell.execute_reply":"2024-07-22T05:26:40.222503Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[1 2 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\nvideo_dir = '/kaggle/input/crash-1500/Crash-1500'\nframes_dir = '/kaggle/working/frames' \n\nfor idx, row in annotations.iterrows():\n    video_number = row['Video Number']\n    video_path = os.path.join(video_dir, f'{video_number:06}.mp4')\n    output_dir = os.path.join(frames_dir, f'{video_number:06}')\n    os.makedirs(output_dir, exist_ok=True)\n\n    ffmpeg_command = f'ffmpeg -i \"{video_path}\" -vf fps=4 \"{output_dir}/frame%04d.png\"'\n#     print(f\"Running FFmpeg command: {ffmpeg_command}\")\n    os.system(ffmpeg_command)\n    \n    extracted_frames = os.listdir(output_dir)\n    print(f\"Extracted {len(extracted_frames)} frames for video {video_number}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import ImageFilter,ImageEnhance\nimport cv2\nimport numpy as np\n\ndef denoise_frame(frame):\n    frame = np.array(frame)\n    denoised_frame = cv2.medianBlur(frame, 5)\n    return Image.fromarray(denoised_frame)\n\ndef deblur_frame(frame):\n    return frame.filter(ImageFilter.UnsharpMask(radius=2, percent=150, threshold=3))\n\ndef enhance_contrast(frame):\n    frame = np.array(frame)\n    lab = cv2.cvtColor(frame, cv2.COLOR_RGB2LAB)\n    l, a, b = cv2.split(lab)\n    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n    cl = clahe.apply(l)\n    limg = cv2.merge((cl, a, b))\n    enhanced_frame = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n    return Image.fromarray(enhanced_frame)\n\ndef sharpen_frame(frame, factor=2.0):\n    enhancer = ImageEnhance.Sharpness(frame)\n    return enhancer.enhance(factor)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:39:59.720609Z","iopub.execute_input":"2024-07-22T05:39:59.721457Z","iopub.status.idle":"2024-07-22T05:39:59.898221Z","shell.execute_reply.started":"2024-07-22T05:39:59.721425Z","shell.execute_reply":"2024-07-22T05:39:59.897301Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def compute_optical_flow(prev_frame, next_frame):\n    # Convert the frames to grayscale for optical flow computation\n    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_RGB2GRAY)\n    next_gray = cv2.cvtColor(next_frame, cv2.COLOR_RGB2GRAY)\n    \n    # Compute optical flow using Farneback method\n    flow = cv2.calcOpticalFlowFarneback(prev_gray, next_gray, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n    return flow","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:40:04.767104Z","iopub.execute_input":"2024-07-22T05:40:04.768126Z","iopub.status.idle":"2024-07-22T05:40:04.774281Z","shell.execute_reply.started":"2024-07-22T05:40:04.768082Z","shell.execute_reply":"2024-07-22T05:40:04.773401Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nframes_dir = '/kaggle/working/frames'\nvideo_dirs = next(os.walk(frames_dir))[1]\nnum_videos = len(video_dirs)\n\nprint(f\"Number of videos with extracted frames: {num_videos}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:40:08.506737Z","iopub.execute_input":"2024-07-22T05:40:08.507347Z","iopub.status.idle":"2024-07-22T05:40:08.515352Z","shell.execute_reply.started":"2024-07-22T05:40:08.507317Z","shell.execute_reply":"2024-07-22T05:40:08.514244Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Number of videos with extracted frames: 1415\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom torch.utils.data import Dataset\nfrom torchvision.io import read_image\nfrom torchvision import transforms\nfrom PIL import Image\nimport os\nimport glob\nimport torch\n\nclass VideoDataset(Dataset):\n    def __init__(self, annotations_df, root_dir, transform=None, num_frames=20):\n        self.annotations = annotations_df\n        self.annotations['Severity of the Crash'] = self.annotations['Severity of the Crash'].str.lower()\n        self.root_dir = root_dir\n        self.transform = transform\n        self.label_map = {'minor': 0, 'moderate': 1, 'severe': 2}\n        self.num_frames = num_frames\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        video_number = self.annotations.iloc[idx]['Video Number']\n        label = self.label_map[self.annotations.iloc[idx]['Severity of the Crash']]\n        frame_dir = os.path.join(self.root_dir, f'{video_number:06}')\n        frames = []\n        optical_flows = []\n        frame_paths = sorted(glob.glob(os.path.join(frame_dir, '*.png')))\n\n        for i, frame_path in enumerate(frame_paths):\n            # Read the frame as a PIL image, convert to RGB, and then to numpy array\n            frame = Image.open(frame_path).convert('RGB')  # Ensure the frame is in RGB format\n            frame = denoise_frame(frame)\n            frame = sharpen_frame(frame)\n            frame = enhance_contrast(frame)\n            frame = deblur_frame(frame)\n\n            # Apply transformations\n            if self.transform:\n                frame = self.transform(frame)\n\n            # Convert the frame tensor to a numpy array\n            frame = frame.numpy().transpose(1, 2, 0)  # Convert from (C, H, W) to (H, W, C)\n            frames.append(frame)\n\n            # Compute optical flow if there is a previous frame\n            if i > 0:\n                prev_frame = frames[i-1]\n                next_frame = frames[i]\n                flow = compute_optical_flow(prev_frame, next_frame)\n                optical_flows.append(flow)\n\n        # If there are fewer optical flows than frames, pad with zeros\n        while len(optical_flows) < len(frames):\n            optical_flows.append(np.zeros_like(optical_flows[0]))\n\n        frames = torch.tensor(np.stack(frames)).permute(0, 3, 1, 2)  # Change to (num_frames, channels, height, width)\n        optical_flows = torch.tensor(np.stack(optical_flows)).permute(0, 3, 1, 2)  # Change to (num_frames-1, channels, height, width)\n\n        return (frames, optical_flows), label\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize to 224x224\n    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n    transforms.RandomRotation(10),  # Random rotation\n    transforms.ToTensor(),  # Convert to tensor and scale pixel values to [0, 1]\n#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = VideoDataset(annotations_df=annotations, root_dir='/kaggle/working/frames', transform=transform)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:40:13.743655Z","iopub.execute_input":"2024-07-22T05:40:13.744366Z","iopub.status.idle":"2024-07-22T05:40:18.721087Z","shell.execute_reply.started":"2024-07-22T05:40:13.744333Z","shell.execute_reply":"2024-07-22T05:40:18.720209Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import TimesformerModel, TimesformerConfig\nimport torch.nn as nn\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:40:22.727205Z","iopub.execute_input":"2024-07-22T05:40:22.727724Z","iopub.status.idle":"2024-07-22T05:40:23.632697Z","shell.execute_reply.started":"2024-07-22T05:40:22.727692Z","shell.execute_reply":"2024-07-22T05:40:23.631882Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class VideoClassifier(nn.Module):\n    def __init__(self, num_classes, num_frames, hidden_size=768):\n        super().__init__()\n        self.config = TimesformerConfig(\n            num_frames=num_frames,\n            num_classes=num_classes,\n            patch_size=16,\n            hidden_size=hidden_size\n        )\n        self.timesformer = TimesformerModel(self.config)\n        self.classifier = nn.Linear(self.config.hidden_size, num_classes)  # Input size is now the hidden size of Timesformer\n        self.quant = torch.quantization.QuantStub()\n        self.dequant = torch.quantization.DeQuantStub()\n        \n    def forward(self, frames, flows):\n        if frames.is_quantized:\n            frames = frames.dequantize()\n        if flows.is_quantized:\n            flows = flows.dequantize()\n#         frames = self.quant(frames)\n#         flows = self.quant(flows)\n#         print(\"Frames shape:\", frames.shape)      # Should be (batch_size, num_frames, channels, height, width)\n#         print(\"Flows shape:\", flows.shape)        # Should be (batch_size, num_frames, channels, height, width)\n\n        # Ensure that the number of frames and channels are consistent\n#         assert frames.size(1) == flows.size(1), \"Mismatch in number of frames between frames and optical flows\"\n\n        # Pad flows if necessary\n        flows_padded = torch.zeros((frames.size(0), frames.size(1), 3, frames.size(3), frames.size(4)), device=frames.device)\n        flows_padded[:, :, :flows.size(2), :, :] = flows  # Copy flows into the first 2 channels of flows_padded\n\n        # Check shapes after padding\n#         print(\"Flows padded shape:\", flows_padded.shape)\n\n        # Concatenate frames and optical flows along the channel dimension\n        combined_input = torch.cat((frames, flows_padded), dim=1)  # Concatenate along the channel dimension\n\n        # Check combined_input shape\n#         print(\"Combined input shape:\", combined_input.shape)  # Should be (batch_size, num_frames, combined_channels, height, width)\n\n        outputs = self.timesformer(combined_input)\n        logits = self.classifier(outputs.last_hidden_state[:, 0, :])  # Use the [CLS] token for classification\n        logits = self.dequant(logits)\n        return logits\n\n# Initialize and move the model to the appropriate device\nmodel = VideoClassifier(num_classes=3, num_frames=20)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nqconfig = torch.quantization.QConfig(\n    activation=torch.quantization.MinMaxObserver.with_args(quant_min=0, quant_max=127),\n    weight=torch.quantization.PerChannelMinMaxObserver.with_args(quant_min=-128, quant_max=127)\n)\nmodel.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\nmodel = torch.quantization.prepare_qat(model, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T12:19:41.832998Z","iopub.execute_input":"2024-07-22T12:19:41.833387Z","iopub.status.idle":"2024-07-22T12:19:45.985464Z","shell.execute_reply.started":"2024-07-22T12:19:41.833353Z","shell.execute_reply":"2024-07-22T12:19:45.984716Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class_counts = annotations['label'].value_counts()\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T10:31:17.321441Z","iopub.execute_input":"2024-07-17T10:31:17.322056Z","iopub.status.idle":"2024-07-17T10:31:17.330969Z","shell.execute_reply.started":"2024-07-17T10:31:17.322025Z","shell.execute_reply":"2024-07-17T10:31:17.329985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, SubsetRandomSampler\nfrom collections import Counter\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Seed for reproducibility\nnp.random.seed(42)\n\n# Split the dataset\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices)\n\ntrain_split = int(np.floor(0.7 * dataset_size))\nval_split = int(np.floor(0.2 * dataset_size))\ntest_split = dataset_size - train_split - val_split\n\ntrain_indices = indices[:train_split]\nval_indices = indices[train_split:train_split + val_split]\ntest_indices = indices[train_split + val_split:]\n\nprint(f\"Total dataset size: {dataset_size}\")\nprint(f\"Training set size: {len(train_indices)}\")\nprint(f\"Validation set size: {len(val_indices)}\")\nprint(f\"Test set size: {len(test_indices)}\")\n\n# Compute class weights for weighted sampling\ntrain_labels = [annotations['Severity of the Crash'][idx] for idx in train_indices]\nclass_counts = Counter(train_labels)\ntotal_samples = len(train_labels)\nclass_weights = {cls: total_samples / count for cls, count in class_counts.items()}\nsample_weights = [class_weights[annotations['Severity of the Crash'][idx]] for idx in train_indices]\nsample_weights = torch.tensor(sample_weights, dtype=torch.float)\n\n# Define samplers\ntrain_sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\nval_sampler = SubsetRandomSampler(val_indices)\ntest_sampler = SubsetRandomSampler(test_indices)\n\n# Define data loaders with reduced batch size\ntrain_loader = DataLoader(dataset, batch_size=1, sampler=train_sampler)\nval_loader = DataLoader(dataset, batch_size=1, sampler=val_sampler)\ntest_loader = DataLoader(dataset, batch_size=2, sampler=test_sampler)\n\nprint(\"Total effective samples in training set:\", len(train_loader) * train_loader.batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:43:42.724826Z","iopub.execute_input":"2024-07-22T05:43:42.725472Z","iopub.status.idle":"2024-07-22T05:43:42.762331Z","shell.execute_reply.started":"2024-07-22T05:43:42.725439Z","shell.execute_reply":"2024-07-22T05:43:42.761601Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Total dataset size: 1415\nTraining set size: 990\nValidation set size: 283\nTest set size: 142\nTotal effective samples in training set: 990\n","output_type":"stream"}]},{"cell_type":"code","source":"# Verify the class distribution in the training set\nclass_counts = {0: 0, 1: 0, 2: 0}\nfor data in train_loader:\n    inputs, labels = data\n    for label in labels:\n        class_counts[label.item()] += 1\n\nprint(\"Class distribution in the training set:\")\nprint(class_counts)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T13:50:37.794885Z","iopub.status.idle":"2024-07-16T13:50:37.795395Z","shell.execute_reply.started":"2024-07-16T13:50:37.795111Z","shell.execute_reply":"2024-07-16T13:50:37.795131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nnum_epochs = 4\nbest_val_acc = 0.0\nsave_path = '/kaggle/working/best_model.pth'\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for data in train_loader:\n        (inputs, flows), labels = data\n        inputs, flows, labels = inputs.to(device), flows.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs, flows)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in val_loader:\n            (inputs, flows), labels = data\n            inputs, flows, labels = inputs.to(device), flows.to(device), labels.to(device)\n            outputs = model(inputs, flows)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_acc = 100 * correct / total\n\n    print('[%d] loss: %.3f, val_acc: %.3f' % (epoch + 1, running_loss / len(train_loader), val_acc))\n\n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(model.state_dict(), save_path)\n        print(f\"New best model saved with val_acc: {best_val_acc:.3f}\")\n\n# Convert the model to a quantized version\nmodel.cpu()\nmodel = torch.quantization.convert(model, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-22T05:44:54.820258Z","iopub.execute_input":"2024-07-22T05:44:54.820638Z","iopub.status.idle":"2024-07-22T11:47:40.967863Z","shell.execute_reply.started":"2024-07-22T05:44:54.820613Z","shell.execute_reply":"2024-07-22T11:47:40.966721Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/ReduceAllOps.cpp:72.)\n  return torch.fused_moving_avg_obs_fake_quant(\n/opt/conda/lib/python3.10/site-packages/torch/ao/quantization/fake_quantize.py:343: UserWarning: _aminmax is deprecated as of PyTorch 1.11 and will be removed in a future release. Use aminmax instead. This warning will only appear once per process. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorCompare.cpp:677.)\n  return torch.fused_moving_avg_obs_fake_quant(\n","output_type":"stream"},{"name":"stdout","text":"[1] loss: 3.950, val_acc: 24.028\nNew best model saved with val_acc: 24.028\n[2] loss: 1.139, val_acc: 34.276\nNew best model saved with val_acc: 34.276\n[3] loss: 1.226, val_acc: 41.343\nNew best model saved with val_acc: 41.343\n[4] loss: 1.174, val_acc: 38.163\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Initialize the model\nmodel = VideoClassifier(num_classes=3, num_frames=20)\ndevice = torch.device('cpu')  # Use CPU for quantized model evaluation\nmodel.to(device)  # Move model to CPU\n\n# Load the pre-trained quantized model\nstate_dict = torch.load('/kaggle/working/best_model.pth')\n\n# Load state dict into model with strict=False to ignore missing keys\nmodel.load_state_dict(state_dict, strict=False)\n\n# Convert the model to a quantized version (assuming it has been trained with quantization aware training)\nmodel = torch.quantization.convert(model, inplace=True)\n\n# Set the model to evaluation mode\nmodel.eval()\n\n\n# Evaluate the model\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for data in test_loader:\n        (inputs, flows), labels = data  # Unpack inputs and optical flows\n        inputs, flows, labels = inputs.to(device), flows.to(device), labels.to(device)\n\n        # Pass the data through the quantized model\n        outputs = model(inputs, flows)  # Model is quantized\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_acc = 100 * correct / total\nprint(f'Test Accuracy: {test_acc:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-07-22T12:40:30.057311Z","iopub.execute_input":"2024-07-22T12:40:30.05803Z","iopub.status.idle":"2024-07-22T13:14:13.710932Z","shell.execute_reply.started":"2024-07-22T12:40:30.057994Z","shell.execute_reply":"2024-07-22T13:14:13.709533Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the best model\n# model = VideoClassifier(num_classes=3, num_frames=20)\nmodel.load_state_dict(torch.load(save_path))\nmodel.eval()\n\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for data in test_loader:\n        (inputs, flows), labels = data  # Unpack inputs and optical flows\n\n        inputs, flows, labels = inputs.to('cpu'), flows.to('cpu'), labels.to('cpu')\n\n        # Pass the data through the model directly\n        outputs = model(inputs, flows)  # Model is not quantized\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\ntest_acc = 100 * correct / total\nprint('Test Accuracy: %.3f' % test_acc)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPUtil\n\nimport torch\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n\ndef free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n\nfree_gpu_cache()                           \n","metadata":{"execution":{"iopub.status.busy":"2024-07-21T10:40:50.27712Z","iopub.execute_input":"2024-07-21T10:40:50.277473Z","iopub.status.idle":"2024-07-21T10:41:03.09237Z","shell.execute_reply.started":"2024-07-21T10:40:50.277445Z","shell.execute_reply":"2024-07-21T10:41:03.091293Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Requirement already satisfied: GPUtil in /opt/conda/lib/python3.10/site-packages (1.4.0)\nInitial GPU Usage\n| ID | GPU | MEM |\n------------------\n|  0 |  0% |  1% |\n|  1 |  0% |  0% |\nGPU Usage after emptying the cache\n| ID | GPU | MEM |\n------------------\n|  0 |  2% |  1% |\n|  1 |  0% |  0% |\n","output_type":"stream"}]},{"cell_type":"code","source":"# import torch\n# from transformers import VideoMAEConfig, VideoMAEImageProcessor, VideoMAEForVideoClassification\n# import torch.nn as nn\n# import torch.optim as optim\n\n# class VideoClassifier(nn.Module):\n#     def __init__(self, num_classes, num_frames):\n#         super().__init__()\n#         self.num_frames = num_frames\n#         self.image_processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n#         self.config = VideoMAEConfig.from_pretrained(\"MCG-NJU/videomae-base\")\n#         self.config.num_frames = num_frames  # Set the number of frames in the configuration\n#         self.config.num_labels = num_classes  # Ensure the number of classes is set here\n#         self.model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base\", config=self.config)\n\n#     def forward(self, x):\n#         x = x.squeeze(0)\n#         x = x.permute(1, 0, 2, 3)\n        \n#         # Forward pass through VideoMAEForVideoClassification model\n#         outputs = self.model(pixel_values=x.unsqueeze(0))\n#         return outputs.logits\n\n\n# model = VideoClassifier(num_classes=3, num_frames=20)\n# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# model.to(device)","metadata":{},"execution_count":null,"outputs":[]}]}